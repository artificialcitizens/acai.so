import { Meta, Canvas } from '@storybook/blocks';
import * as ModelStories from '../1-models/models.stories';
import * as IndexStories from './indexes.stories';

<Meta title="Indexes/Ingest" />

# Ingesting Data

Once we've chosen and loaded our data store we'll need a way to *ingest* data to 
store for later retrieval. We'll need to chunk and vectorize our data so we can store it.

Lets see how.

## Text Splitters

Language Models are often limited by the amount of text that you can pass to them. 
Therefore, it is necessary to split them up into smaller chunks. 
LangChain provides several utilities for doing so.

Using a Text Splitter can also help improve the results from vector store searches, 
as eg. smaller chunks may sometimes be more likely to match a query. 
Testing different chunk sizes (and chunk overlap) is a worthwhile exercise 
to tailor the results to your use case.

### RecursiveCharacterTextSplitter

This is the recommended splitter for most use cases. This will split documents 
recursively by different characters starting with "\n\n", then "\n", then " ". 
This tries to keep the semantically relevant parts of the text together.

```js
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);
```

You'll note that in the above example we are splitting a raw text string and getting back a list of documents. 
We can also split documents directly.

```js
import { Document } from "langchain/document";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);
```

## Embeddings

Now that we have our data in reasonable sized chunks, let's convert them into vectors.

Embeddings can be used to create a numerical representation of textual data. 
This numerical representation is useful because it can be used to find similar documents.

```js
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

/* Create instance */
const embeddings = new OpenAIEmbeddings();

/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
/*
[
   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
  ... 1436 more items
]
*/

// We can also embed documents themselves
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);

```

### Try it

Enter some text and see the vector representation of it.

<Canvas of={ModelStories.EmbeddingModelExample} />

## Vector Stores

A vector store is a database designed for storing documents and their embeddings.
We can then query this database to find documents that are similar to a given query.


```js
interface VectorStore {
  /**
   * Add more documents to an existing VectorStore
   */
  addDocuments(documents: Document[]): Promise<void>;

  /**
   * Search for the most similar documents to a query
   */
  similaritySearch(
    query: string,
    k?: number,
    filter?: object | undefined
  ): Promise<Document[]>;

  /**
   * Search for the most similar documents to a query,
   * and return their similarity score
   */
  similaritySearchWithScore(
    query: string,
    k = 4,
    filter: object | undefined = undefined
  ): Promise<[object, number][]>;

  /**
   * Turn a VectorStore into a Retriever
   */
  asRetriever(k?: number): BaseRetriever;

  /**
   * Advanced: Add more documents to an existing VectorStore,
   * when you already have their embeddings
   */
  addVectors(vectors: number[][], documents: Document[]): Promise<void>;

  /**
   * Advanced: Search for the most similar documents to a query,
   * when you already have the embedding of the query
   */
  similaritySearchVectorWithScore(
    query: number[],
    k: number,
    filter?: object
  ): Promise<[Document, number][]>;
}
```

There are several vector stores available to us. In our current pipeline we are leveraging cloud based 
[Pinecone](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pinecone) as our Vectorstore, 
but for the sake of time lets take a look at an in memory solution to learn more about this concept.

### MemoryVectorStore

We can create a new index from a list of strings and some corresponding metadata.

```js
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await MemoryVectorStore.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings()
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
};
```

We can also create indexes directly from a loaded document. 



```js
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

const memoryStoreExample = async () => {
  const splitter = new CharacterTextSplitter({
    separator: '\n',
    chunkSize: 150,
    chunkOverlap: 10,
  });
  // Create docs with a loader
  const loader = new TextLoader('src/stories/assets/documents/vector_store_guide.txt');
  try {
    const docs = await loader.load();
    // Split the docs into chunks
    const splitDocs = await splitter.splitDocuments(docs);
    // lets take a look at a random doc from our collection
    console.log('splitDocs', splitDocs[5]);
  //> splitDocs Document {
  //  pageContent: 'If you come from Python and you were looking for something similar to FAISS, pick HNSWLib',
  //  metadata: {
  //   source: 'src/stories/assets/documents/vector_store_guide.txt',
  //   loc: { lines: [Object] }
  // }
    const vectorStore = await MemoryVectorStore.fromDocuments(
      splitDocs,
      new OpenAIEmbeddings({ openAIApiKey: <APIKEY> }),
    );

    // Search for the most similar document
    const result = await vectorStore.similaritySearch('what if I want something like Python?', 1);

    return result[0].pageContent;
  } catch (error: any) {
    return error.message;
  }
};

const test = await memoryStoreExample();
console.log(test);
```
Here we are loading some text from a file and splitting it into chunks of 150 characters 
with a 10 character overlap, using the "\n" as the point we want to break our text.

We are using the Vector Store choosing guide from the Langchain [docs](https://js.langchain.com/docs/modules/indexes/vector_stores/#which-one-to-pick) as our example text.

```md
Here's a quick guide to help you pick the right vector store for your use case:

If you're after something that can just run inside your Node.js application, in-memory, without any other servers to stand up, then go for HNSWLib
If you're looking for something that can run in-memory in browser-like environments, then go for MemoryVectorStore
If you come from Python and you were looking for something similar to FAISS, pick HNSWLib
If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for Chroma
If you're using Supabase already then look at the Supabase vector store to use the same Postgres database for your embeddings too
If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for Pinecone
```

<strong>Note</strong> Storybook has a node env conflict with a method in this function so no cool demo here
open your terminal and run: `ts-node-esm src/scripts.ts` to see the results

{/* <Canvas of={IndexStories.MemoryStoreExample} /> */}