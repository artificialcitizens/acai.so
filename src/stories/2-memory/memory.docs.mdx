import { Meta, Canvas } from '@storybook/blocks';
import * as MemoryStories from './memory.stories';

<Meta title="Memory/Overview" />

# Memory

[Memory](https://js.langchain.com/docs/modules/memory/) gives us wrappers to manage the contextual memory
of our models. This allows us to create an ongoing interaction with the user.

## Basic Memory

Ultimately we are just keeping track of the conversation and sending all of the history to the model as one query.

Lets take a look at this simple chat component that shows how this can be managed without a memory wrapper

```js
import { OpenAI } from 'langchain/llms/openai';

const model = new OpenAI();
chatHistory = '';

const chat = async (input: string, chatHistory: string) => {
  const response = model.call(chatHistory + `\n` + input + '\n');
  return response;
};
```

### Try it out

Each call is passing in the entirety of the chat history, original prompt, etc and the end user only
sees the last message.

Check your dev console to see the full chat history

<Canvas of={MemoryStories.BasicMemoryExample} />

## BufferWindowMemory

The BufferWindowMemory is a simple wrapper that allows us to keep track of the last N interactions

```ts
import { OpenAI } from 'langchain/llms/openai';
import { BufferWindowMemory } from 'langchain/memory';
import { ConversationChain } from 'langchain/chains';

const model = new OpenAI({});
const memory = new BufferWindowMemory({ k: 5 });
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
